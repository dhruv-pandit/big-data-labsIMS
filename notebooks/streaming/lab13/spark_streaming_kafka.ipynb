{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integration of Apache Kafka and Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give some context to this notebook. An alien entity has entered Portugal and suddenly starts turning citizens into scotsmen! Fear grips the country. We need to find out what is happening and how it is affecting the population.\n",
    "\n",
    "[To understand what is going on](https://www.youtube.com/watch?v=qxDJMn-534Y)\n",
    "(This is a Monty **Python** sketch)\n",
    "\n",
    "We have been charged by the Portuguese government to analyze how the population has already changed. We will use **Spark Streaming** to stream data from our Kafka cluster and analyze live-streamed epidemiological data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with streams, stream-stream joins and stream-static joins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<img src=\"img/scottish_portugal.png\" height=\"500\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured Streaming treats a ``stream`` of data as a table that is updated in real time. An underlying process then regularly checks for updates and updates the table, if necessary. The API around Structured Streaming is designed in such a way that what works on your DataFrame, should also work on your streamed DataFrame! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Spark Streaming`` is a subset of Spark's functionalities that allows us to work with event-based data, as with our Kafka cluster. We set some global variables and import Schema Types to **structure our data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, StructField, IntegerType, TimestampType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"localhost:8098\"\n",
    "KAFKA_TOPIC = \"scotsmen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a Spark Session. We import the ``Spark SQL Kafka Connector`` as a dependency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize local spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"kafka_streaming\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0') \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\",\"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.bindAddress\",\"127.0.0.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ``subscribe-publish`` paradigm, we subscribe to the Kafka topic ``scotsmen``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from ``KAFKA_TOPIC``\n",
    "streaming_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the schema of the messages received\n",
    "scotsmen_schema = StructType([\n",
    "  StructField(\"district\", StringType()),\n",
    "  StructField(\"new_scotsmen\", IntegerType()),\n",
    "  StructField(\"timestamp\", TimestampType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select the 'value' column and cast it as a String\n",
    "json_df = streaming_df.select(\n",
    "    F.from_json(F.col(\"value\").cast(\"string\"), scotsmen_schema).alias(\"value\"), \n",
    "    \"timestamp\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We instantiate an SQL view to inspect our data\n",
    "json_df.select(\"value.*\").createOrReplaceTempView(\"scotsmen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample query from ``scotsmen`` table\n",
    "scotsmen_query = spark.sql(\"SELECT * FROM scotsmen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, as with the non-streaming API, there are ``transformations`` and ``actions``. Execution of a query operation on Spark Streaming is lazy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Sources & Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Structured Streaming supports different input sources and sinks. Supported sinks are:\n",
    "1. Kafka Streams\n",
    "2. Files on a distributed file system (HDFS, S3). Spark will read files from a directory\n",
    "3. A Socket Source\n",
    "\n",
    "While input sources specify the origin of the data, sinks specify where the data will be written. Those sinks can be:\n",
    "1. Kafka sink: Pushes data to Kafka\n",
    "2. Files sink: Writes the output to a file (JSON, parquet, CSV etc.)\n",
    "3. ForEach sink: Can be used to for each row of a DataFrame for custom storage logic\n",
    "4. Console sink: Used for testing\n",
    "5. Memory: Used for debugging\n",
    "\n",
    "``Memory`` and ``Console``sinks are very similar. ``Memory`` mode makes the data available in an in-memory table for interactive inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 10:12:47 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/nt/03y4p9md50gblp_0svv74zb80000gn/T/temporary-e40bba87-ac59-43e0-b96e-2ffe3b0b966e. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/12/03 10:12:47 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 10:12:47 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=34276Kb max_used=35295Kb free=96795Kb\n",
      " bounds [0x000000010d1f8000, 0x000000010f538000, 0x00000001151f8000]\n",
      " total_blobs=12479 nmethods=11478 adapters=913\n",
      " compilation: disabled (not enough contiguous free space left)\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------+------------+---------+\n",
      "|district|new_scotsmen|timestamp|\n",
      "+--------+------------+---------+\n",
      "+--------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This will print the results to the console\n",
    "query = scotsmen_query.writeStream.outputMode(\"append\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping the query\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three different output modes available. Here, we used ``append``, which only adds new records to the sink. The other two are ``update`` and ``complete``. ``update`` mode updates the data in the sink, while ``complete`` mode replaces the data in the sink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 10:12:54 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/nt/03y4p9md50gblp_0svv74zb80000gn/T/temporary-f7fd2128-60af-41d1-ac26-535d6e9d1e37. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/12/03 10:12:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot start query with name scotsmen_table as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Write this query to a memory table\u001b[39;00m\n\u001b[1;32m      2\u001b[0m memory_query \u001b[38;5;241m=\u001b[39m \u001b[43mscotsmen_query\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscotsmen_table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/sql/streaming/readwriter.py:1527\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot start query with name scotsmen_table as a query with that name is already active in this SparkSession"
     ]
    }
   ],
   "source": [
    "# Write this query to a memory table\n",
    "memory_query = scotsmen_query \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"scotsmen_table\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|district|total_new_scotsmen|\n",
      "+--------+------------------+\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT district, sum(new_scotsmen) AS total_new_scotsmen FROM scotsmen_table GROUP BY district\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert JSON to DataFrame\n",
    "new_scotsmen_df = json_df.select(\n",
    "    F.col(\"value.district\").alias(\"conversion_district\"),\n",
    "    F.col(\"value.timestamp\").alias(\"conversion_timestamp\"),\n",
    "    F.col(\"value.new_scotsmen\").alias(\"new_scotsmen\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watermarking ensures ensures that late events \n",
    "# (up to 30 seconds after their event timestamp) \n",
    "# are considered in the aggregation, but any event arriving after that will be ignored.\n",
    "windowed_df = new_scotsmen_df.withWatermark(\"conversion_timestamp\", \"30 seconds\") \\\n",
    "    .groupBy(\n",
    "        F.window(new_scotsmen_df[\"conversion_timestamp\"], \"3 minute\"),  # 3-minute window\n",
    "        new_scotsmen_df[\"conversion_district\"]  # Group by district\n",
    "    ) \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"event_count\"),   # Count events in each window for each district\n",
    "        F.sum(\"new_scotsmen\").alias(\"total_new_scotsmen\"),  # Sum of values for each district in each window\n",
    "        F.avg(\"new_scotsmen\").alias(\"average_new_scotsmen\") # Compute the average value for each district in each window\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 10:11:29 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/nt/03y4p9md50gblp_0svv74zb80000gn/T/temporary-78aa0eb1-1149-49df-b600-304e7f2e1229. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/12/03 10:11:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/03 10:11:29 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    }
   ],
   "source": [
    "window_query = windowed_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"new_scotsmen_aggregated\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-----------+------------------+--------------------+\n",
      "|window|conversion_district|event_count|total_new_scotsmen|average_new_scotsmen|\n",
      "+------+-------------------+-----------+------------------+--------------------+\n",
      "+------+-------------------+-----------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM new_scotsmen_aggregated\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured Streaming supports ``Joins``. This means that you are able to (I) join a stream with a static DataFrame and (II) join two streams. This can be used to supplement streaming data with another data source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will supplement our ``scotsmen`` table with the ``bag_pipes_sales`` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we need to read from Kafka.\n",
    "# This time, we subscribe to the bagpipes topic\n",
    "bagpipes_stream = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"subscribe\", \"bagpipe\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the schema of the messages received\n",
    "bagpipes_schema = StructType([\n",
    "  StructField(\"district\", StringType()),\n",
    "  StructField(\"bagpipe_sales\", IntegerType()),\n",
    "  StructField(\"timestamp\", TimestampType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We select the 'value' column and cast it as a String\n",
    "bagpipes_json_df = bagpipes_stream.select(\n",
    "    F.from_json(F.col(\"value\").cast(\"string\"), bagpipes_schema).alias(\"value\"), \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the JSON to DataFrame and alias columns\n",
    "bagpipe_df = bagpipes_json_df.select(\n",
    "    F.col(\"value.district\").alias(\"sales_district\"),\n",
    "    F.col(\"value.timestamp\").alias(\"sale_timestamp\"),\n",
    "    F.col(\"value.bagpipe_sales\").alias(\"bagpipe_sales\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add watermarking to both streams to handle late data\n",
    "conversion_stream = new_scotsmen_df.withColumn(\"conversion_truncated_timestamp\", F.date_trunc(\"minute\", new_scotsmen_df[\"conversion_timestamp\"]))\n",
    "bagpipes_stream = bagpipe_df.withColumn(\"sales_truncated_timestamp\", F.date_trunc(\"minute\", bagpipe_df[\"sale_timestamp\"]))\n",
    "\n",
    "# Watermark the datasets\n",
    "conversion_stream = conversion_stream.withWatermark(\"conversion_truncated_timestamp\", \"1 minute\")\n",
    "bagpipes_stream = bagpipes_stream.withWatermark(\"sales_truncated_timestamp\", \"1 minute\")\n",
    "\n",
    "# Alias the datasets\n",
    "conversion_stream = conversion_stream.alias(\"s1\")\n",
    "bagpipes_stream = bagpipes_stream.alias(\"s2\")\n",
    "\n",
    "# Perform the join between the two windowed streams on 'district' and matching \n",
    "# windows by using a functional expression\n",
    "joined_stream = conversion_stream \\\n",
    "    .join(\n",
    "        bagpipes_stream,\n",
    "        F.expr(\"\"\"\n",
    "            s1.conversion_district = s2.sales_district AND\n",
    "            s2.sales_truncated_timestamp >= s1.conversion_truncated_timestamp AND\n",
    "            s2.sales_truncated_timestamp <= s1.conversion_truncated_timestamp + interval 5 minute\n",
    "        \"\"\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        \"s2.sales_truncated_timestamp\",\n",
    "        \"s1.new_scotsmen\",\n",
    "        \"s2.bagpipe_sales\",\n",
    "        \"s1.conversion_district\"\n",
    "    )\n",
    "\n",
    "# Create the window column before aggregation\n",
    "joined_stream = joined_stream.withColumn(\"window\", F.window(\"sales_truncated_timestamp\", \"1 hour\"))\n",
    "\n",
    "# Aggregate values per district\n",
    "aggregated_stream = joined_stream \\\n",
    "    .groupBy(\n",
    "        joined_stream.conversion_district,\n",
    "        joined_stream.window\n",
    "    ) \\\n",
    "    .agg(\n",
    "        F.sum(joined_stream.new_scotsmen).alias(\"total_new_scotsmen\"),\n",
    "        F.sum(joined_stream.bagpipe_sales).alias(\"total_bagpipe_sales\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results to the console for inspection\n",
    "query = aggregated_stream \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"new_scotsmen_bagpipe_sales_per_district\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the results\n",
    "spark.sql(\"SELECT * FROM new_scotsmen_bagpipe_sales_per_district\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static-Stream Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from joining two streams, Spark also supports joining a stream with a static DataFrame. This can be used to supplement streaming data with another data source, such as a lookup table. Here  we will supplement our ``conversation_stream`` with the ``portugal_district_population2022.csv`` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_schema = StructType([\n",
    "  StructField(\"district\", StringType()),\n",
    "  StructField(\"pop\", IntegerType())\n",
    "])\n",
    "\n",
    "population_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .schema(population_schema) \\\n",
    "    .load(\"portugal_district_population2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the conversion stream with the population data\n",
    "joined_stream_population = conversion_stream.join(\n",
    "    population_df,\n",
    "    conversion_stream.conversion_district == population_df.district,\n",
    "    \"inner\" \n",
    ") \\\n",
    "    .withColumn(\n",
    "        \"conversions_per_pop\", F.col(\"new_scotsmen\") / F.col(\"pop\")\n",
    "    )\n",
    "\n",
    "# Select the columns you need\n",
    "result_stream = joined_stream_population.select(\n",
    "    \"conversion_truncated_timestamp\",\n",
    "    \"new_scotsmen\",\n",
    "    \"conversion_district\",\n",
    "    \"pop\",\n",
    "    \"conversions_per_pop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the results to the console for testing\n",
    "query = result_stream \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"prop_pop_converted\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the window duration and slide duration\n",
    "window_duration = \"1 hour\"\n",
    "slide_duration = \"10 minutes\"\n",
    "\n",
    "# SQL query to compute the cumulative sum of the ratio\n",
    "sql_query = f\"\"\"\n",
    "SELECT\n",
    "    window.start AS window_start,\n",
    "    window.end AS window_end,\n",
    "    SUM(conversions_per_pop) AS cumulative_ratio,\n",
    "    conversion_district\n",
    "FROM (\n",
    "    SELECT\n",
    "        conversions_per_pop,\n",
    "        window(current_timestamp(), '{window_duration}', '{slide_duration}') AS window,\n",
    "        conversion_district\n",
    "    FROM prop_pop_converted\n",
    ")\n",
    "GROUP BY conversion_district, window\n",
    "ORDER BY window_start\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "result_df = spark.sql(sql_query)\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulating Streaming Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to \"simulate\" a streaming dataset by reading from a directory of CSV files. This can be useful for testing and debugging purposes. The dataset contains individual files that can be read as a batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the weather schema\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"min_temperature\", DoubleType(), True),\n",
    "    StructField(\"max_temperature\", DoubleType(), True),\n",
    "    StructField(\"precipitation\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Path to the directory containing the CSV files\n",
    "# Note that this must be a directory and not an individual file\n",
    "input_path = \"weather\"\n",
    "\n",
    "NUM_FILES_PER_TRIGGER = 3\n",
    "\n",
    "# Read the streaming DataFrame from the directory\n",
    "streaming_df = spark.readStream \\\n",
    "    .option(\"maxFilesPerTrigger\", NUM_FILES_PER_TRIGGER) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(input_path)\n",
    "\n",
    "# Define the query to process the streaming data\n",
    "# It is possible to set the batch size \n",
    "# to control how frequently the streaming query processes new data.\n",
    "# This is done using the trigger option in the writeStream method. \n",
    "# The trigger option allows you to specify the processing time interval, \n",
    "# which determines the batch size.\n",
    "# maxFilesPerTrigger is the maximum number of files that will be\n",
    "# processed in a single trigger.\n",
    "query = streaming_df.writeStream \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .option(\"maxFilesPerTrigger\", 5) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"weather\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now query the static dataset just like we did before\n",
    "spark.sql(\n",
    "    \"\"\"SELECT max_temperature, min_temperature, timestamp \n",
    "    FROM weather ORDER BY timestamp \n",
    "    DESC\n",
    "    \"\"\"\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
