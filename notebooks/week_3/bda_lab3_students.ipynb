{"cells":[{"cell_type":"markdown","metadata":{"id":"mwjiD99ea65F"},"source":["# Big Data - Advanced  Map-Reduce Exercises"]},{"cell_type":"markdown","metadata":{"id":"wpa_xnU-a65G"},"source":["In this notebook we are going to look at map reduce jobs. We will run through the map reduce logic entirely in python so you can see what is happening at each point in time.\n","\n","IMPORTANT: This notebook exists in the container and you will lose any changes when you close the container!\n","To keep a copy for reference you can go to menu bar above and select File->Download as->Notebook and save to your laptop."]},{"cell_type":"markdown","metadata":{"id":"WqhWaBJ8a65G"},"source":["First we will move the articles file into the hdfs. Here we will be using the two approaches to using BASH in Jupyter Notebooks both the ! symbol and the magic symbol %%. First check the file articles.part exists in the container by running the code below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"60xBNMqMa65G"},"outputs":[],"source":["!ls"]},{"cell_type":"markdown","metadata":{"id":"X81pnrnBa65G"},"source":["# Python mapper and reducer"]},{"cell_type":"markdown","metadata":{"id":"Ph_zmbY_a65H"},"source":["In this example we will simulate the map and redcue phases entirely in Python. This allows us to run MapReduce jobs quickly without having to wait for the distributed filesystem."]},{"cell_type":"markdown","metadata":{"id":"YBiosxx-a65H"},"source":["In this example we will be see python code that reads article and uses tupples to hold key value pairs (this makes it easier for us to sort).\n","\n","The next cell will pull the head and tail of our data to local files for us to work with. This allows us to simulate that we have a large file split into two parts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoN6IIHsa65H"},"outputs":[],"source":["%%bash\n","cat articles.part | head -n 5 > part1.txt\n","cat articles.part | tail -n 2 > part2.txt"]},{"cell_type":"markdown","metadata":{"id":"CflOjEVEa65H"},"source":["We have split our articles files into two parts, to represent how large files get split in the hdfs files system."]},{"cell_type":"markdown","metadata":{"id":"W7yAcuvoa65H"},"source":["Use bash to check if the two new files have been added."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cubh5DsKa65H"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"VTzCDyYba65H"},"source":["Check the contentrs of one of the files using bash. Make sure the file contain wikipedia aritcles. Try to see if you can understand how the data is structured, each article is a single 'line' in the file, it start with an ID number followed by a tab and then the full article text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VeKRs0Hva65H"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"jbRvWRuaa65H"},"source":["Let's define three simple python functions to replicate the functionality we need. A Mapper, a sort, and a reducer.\n","\n","Make sure you understant what each of these are doing.\n","\n","The Mapper uses regular expression, to split the lines into words, for more"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1xmk7Ana65H"},"outputs":[],"source":["import string\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zngrbQmna65H"},"outputs":[],"source":["def word_count_mapper(wiki_lines):\n","    # List to store the kep value tuples the mapper will return\n","    key_value_pairs = []\n","\n","    # Iterate through lines in the input file\n","    for line in wiki_lines:\n","        # Seperate the article ID and the text into seperate variable\n","        article_id, text = (line.strip()).split('\\t', 1)\n","\n","        # Split the text into words using regular expressions\n","        words = re.split(\"\\W*\\s+\\W*\", text, flags=re.UNICODE)\n","\n","        # For each word add a key value pair to the list, where the key is the word and the value is always 1.\n","        for word in words:\n","            # Remove punctuation and make word lower case\n","            word_stripped = word.lower().translate(str.maketrans('', '', string.punctuation))\n","\n","            #Add word to results\n","            key_value_pairs.append((word_stripped, 1))\n","\n","    return key_value_pairs"]},{"cell_type":"markdown","metadata":{"id":"nHl8Phnoa65H"},"source":["Lets apply our mapper to a simple example to check what it is doing:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Q35mTg2a65H"},"outputs":[],"source":["map_result = word_count_mapper([\"ArticleID\tHello I'm Bob. Hello Bob, I'm Greg.\"])\n","map_result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l4VlNK99a65H"},"outputs":[],"source":["def shuffle_sort(multiple_key_value_pair_lists):\n","    sorted_kvp = []\n","\n","    # We need to sort all key value pairs together from multiple lists of key value pairs\n","    for kvp_list in multiple_key_value_pair_lists:\n","\n","        #Add each key value pair to a single list\n","        for kvp in kvp_list:\n","            sorted_kvp.append(kvp)\n","\n","    # Sort the list, the argument here says we want to use the first part of the tupple (the word) as the sorting criteria\n","    sorted_kvp.sort(key=lambda tup: tup[0])\n","\n","    return sorted_kvp"]},{"cell_type":"markdown","metadata":{"id":"xzw6Yzfya65I"},"source":["Let's test the shuffle and sort phase:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fkb-Y_jBa65I"},"outputs":[],"source":["sorted_result = shuffle_sort([map_result])\n","sorted_result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1-JvH24a65K"},"outputs":[],"source":["def reducer(key_value_pairs):\n","    reduced_key_value_pairs = []\n","    current_key = None\n","    word_sum = 0\n","\n","    # here we iterate through all the key value pairs, if the key is the same as the previous we just add to the count,\n","    # if not we add the word and it's final count to the final list the function will return\n","    total_count = len(key_value_pairs)\n","    for i in range(total_count):\n","        key, count = key_value_pairs[i]\n","        # if the new key is different to the previous (and not None) it is added to our final list and the wordcount reset\n","        if current_key != key:\n","            if current_key:\n","                reduced_key_value_pairs.append((current_key, word_sum))\n","            word_sum = 0\n","            current_key = key\n","\n","        #Add to the counter for the current word\n","        count = int(count)\n","        word_sum += count\n","\n","        #Add the last word\n","        if i + 1 == total_count:\n","            reduced_key_value_pairs.append((key, word_sum))\n","\n","    return reduced_key_value_pairs"]},{"cell_type":"markdown","metadata":{"id":"fNUVWeT0a65K"},"source":["Finally, lets see what the reducer does:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fTDcaMTla65K"},"outputs":[],"source":["reducer(sorted_result)"]},{"cell_type":"markdown","metadata":{"id":"9l3HkYdRa65K"},"source":["There we have our word count result."]},{"cell_type":"markdown","metadata":{"id":"OILmvC5Va65K"},"source":["Would this reducer work if the input list wasn't sorted?"]},{"cell_type":"markdown","metadata":{"id":"NGTO-ogPa65K"},"source":["## Running map reduce"]},{"cell_type":"markdown","metadata":{"id":"jM7f95l9a65K"},"source":["Let's put it all together and simulate the map reduce process with python:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6oXIrCLa65K"},"outputs":[],"source":["# Import data into the variables we will work with\n","part1 = open(\"part1.txt\")\n","part2 = open(\"part2.txt\")\n","file_parts = [part1, part2]\n","\n","# Map phase, the mapper function will run on multiple file parts\n","mapper_result = []\n","\n","for part in file_parts:\n","    mapper_result.append(word_count_mapper(part))\n","\n","# Sort phase running on mapper_result (which is a list of lists of key value pairs)\n","sort_result = shuffle_sort(mapper_result)\n","\n","# Reduce phase on sorted results\n","reduce_result = reducer(sort_result)\n","\n","# Here we sort the results of the reducer so we can find the most common word\n","reduce_result.sort(key=lambda tup: tup[1])\n","\n","# Most common word tuple\n","reduce_result[len(reduce_result)-1]"]},{"cell_type":"markdown","metadata":{"id":"4aODzl_ca65K"},"source":["Predictably 'the' is the most common word in our example. Use python to see the top 20 in the variable above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtrdlyYBa65K"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"5R6TUCkQa65K"},"source":["# Map reduce exercise"]},{"cell_type":"markdown","metadata":{"id":"zx0nsS5Xa65L"},"source":["For this example we are interested in the distribution of the number of words in each sentence that has more than one word (sentences with one word in this dataset are mostly html formatting).\n","\n","Write a new simple python mapper and reducer to accomplish this task. Note we will use the same sort function, so please return a list of key value pairs as tupples from your mapper. Take a look at the simple python mapper above.\n","\n","From your result we want to find:\n","\n","the most words in a sentence,\n","\n","the average sentence length in number of words (the mean sentence length),\n","\n","and the most common sentence length in number of words.\n","\n","Keep this in mind.\n","\n","If you are have trouble try running each fucntion seperately on a simple text to see if the output looks like what you would expect."]},{"cell_type":"markdown","metadata":{"id":"Z1j244NXa65L"},"source":["## Find the most words in a sentence"]},{"cell_type":"markdown","metadata":{"id":"2NlTBmAma65L"},"source":["Fill in the code below to create a mapper and reducer that will what is the maximum number of words in a sentence:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"whhR0utZa65L"},"outputs":[],"source":["def sentence_mapper(wiki_lines):\n","    # List to store the kep value tuples the mapper will return\n","    key_value_pairs = []\n","\n","    # Iterate through lines in the input file\n","    for line in wiki_lines:\n","        # Seperate the article ID and the text into seperate variable\n","        article_id, text = (line.strip()).split('\\t', 1)\n","\n","        # Split the text into sentences\n","        sentences = (text).split('.')\n","\n","        # YOUR CODE HERE.....\n","        # Remember to removes sentence of length < 2\n","\n","\n","    return key_value_pairs"]},{"cell_type":"markdown","metadata":{"id":"5s4ihRNXa65L"},"source":["Test your mapper on the simple example below:"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"fFrxIMnya65L"},"outputs":[],"source":["sentence_map_result = sentence_mapper([\"ArticleID\tHello I'm Bob. Hello Bob, I'm Greg. Nice hair Greg.\"])\n","sentence_map_result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1OYgnmsMa65L"},"outputs":[],"source":["def sentence_reducer(key_value_pairs):\n","    reduced_key_value_pairs = []\n","    current_key = None\n","    word_sum = 0\n","\n","    # here we iterate through all the key value pairs, if the key is the same as the previous we just add to the count,\n","    # if not we add the word and it's final count to the final list the function will return\n","    total_count = len(key_value_pairs)\n","    for i in range(total_count):\n","        # YOUR CODE HERE ......\n","\n","    return reduced_key_value_pairs\n"]},{"cell_type":"markdown","metadata":{"id":"Hi_U1kp5a65O"},"source":["Test your reducer below:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ou-Ow46Ua65O"},"outputs":[],"source":["sentence_reducer(shuffle_sort([sentence_map_result]))"]},{"cell_type":"markdown","metadata":{"id":"Tt10MXtxa65O"},"source":["Lets test your files below.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QeDFLw9a65O"},"outputs":[],"source":["# Import data into the variables we will work with\n","part1 = open(\"part1.txt\")\n","part2 = open(\"part2.txt\")\n","file_parts = [part1, part2]\n","\n","# Map phase, running on all file parts\n","mapper_result_s = []\n","\n","for part in file_parts:\n","    mapper_result_s.append(sentence_mapper(part))\n","\n","# Sort phase running on mapper_result\n","sort_result_s = shuffle_sort(mapper_result_s)\n","\n","# Reduce phase on sorted results\n","reduce_result_s = sentence_reducer(sort_result_s)\n"]},{"cell_type":"markdown","metadata":{"id":"grSkWlY6a65O"},"source":["From your result can you find:\n","\n","the most words in a sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PC-CDeZaa65O"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"7rp2zzCla65O"},"source":["If needed adjust your mapper and reducer to find the following results:"]},{"cell_type":"markdown","metadata":{"id":"WzAXSyFBa65O"},"source":["least words in a sentence (should be more than 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnGxiQS5a65O"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"zhTzqYgGa65O"},"source":["the most common sentence length in number of words and how many sentence have this legnth"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QqJ7f-xaa65O"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"TLlkIH5Qa65O"},"source":["The average sentence length in number of words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLJt2xcva65P"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"AgCeXaZBa65P"},"source":["Say we were just interested in the average number of words in a sentence. Can we just have a map job that calculates the average for each part of the file and then a reduce job that takes the average of these map results? Why / Why not?"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Qt7SgfNEa65P"},"source":["No, the average of the averages for each part would not equal the global average."]},{"cell_type":"markdown","metadata":{"id":"TjZOPJ6za65P"},"source":["Do you think you have minised the amount of information passed to the reducer in each case? Have you performed the maximum quantity of computation at the data?"]},{"cell_type":"markdown","metadata":{"id":"GqejlhbCa65P"},"source":["This question is just to check that you are not passing the full sentence to the reducer.\n","For a number of the questions we could reduce the information passed to the reducer, for example for average words per sentence the only information the reducer needs is the total number of words and total number of sentences in each part."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}