{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6099d71f-0a25-46f0-b523-0c1bc29627bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SparkSQL and DataFrames\n",
    "\n",
    "The majority of the data that a data scientist deals with is either structured or semistructured. The PySparkSQL module is a higher-level abstraction over PySpark Core for processing structured and semistructured datasets. By using PySparkSQL, we can use SQL and HiveQL code too, which makes this module popular among database programmers and Apache Hive users.\n",
    "\n",
    "The APIs provided by PySparkSQL are optimized. PySparkSQL can read data from many file types such as CSV files, JSON files, and files from other databases. In SparkSQL we work with DataFrames instead of RDDs which you may have come across before. The DataFrame abstraction is similar to a table in a relational database management system. The DataFrame consists of named columns and is a collection of Row objects. Row objects are defined in PySparkSQL. It should be noted that in the background the DataFrame is implemented based on RDDs so everything we have learned about RDDs also applies.\n",
    "\n",
    "Todays lab assumes you already familiar with writting SQL queries. If not it is worth spending time to review an online tutorial on SQL. For example: \n",
    "\n",
    "https://www.tutorialspoint.com/sql/index.htm\n",
    "\n",
    "Note: It will take some time to learn SQL if you have never used it before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e633e3b-1d5a-49aa-ab9b-165adba99b76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Dataframes\n",
    "\n",
    "A DataFrame is collection of named columns, let's take a look at how to create a dataframe from some Python data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97bf3347-c3dc-4f9f-9586-d309a7af1b21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Using Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e92cd41-5076-4b78-83c7-2fd0a1868227",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh wget https://raw.githubusercontent.com/UmaMaquinaDeOndas/DataBricks-Tutorials/master/adult.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c0f9bf9-76d5-4ddd-973c-16cac3eebe84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data into the big data cluster as a dataframe\n",
    "df = spark.read.csv('file:/databricks/driver/adult.data',header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you select `\"age\", \"education\", \"income\"` columns from the df, and show them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea5e5b2f-a555-4629-bc7b-bae6be70e713",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Select\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Can you select the rows where the age is greater than 30 and the income is equal to '>50K', and show them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f80a0f6-5713-4e3e-81d6-5ba3f6a7ada9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Where\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you filter the rows where the age is greater than 30 and the income is equal to '>50K', and show them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48f65642-decb-40b4-81d5-e47575e264dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2f4c73-715a-46a8-9c4b-732666636aa4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Agregation in dataframes\n",
    "\n",
    "To get a summarized pattern of data, data scientists perform aggregation on a given\n",
    "dataset. Summarized patterns are easy to understand. Sometimes the summarization is\n",
    "done based on the key. To perform aggregation based on the key, we first need to group\n",
    "the data by key.\n",
    "\n",
    "In PySparkSQL, grouping by key can be performed by using the groupBy()\n",
    "function. This function returns the pyspark.sql.group.GroupedData object. After this\n",
    "GroupedData object is created, we can apply many aggregation functions such as avg(),\n",
    "sum(), count(), min(), max(), and sum() on GroupedData.\n",
    "\n",
    "The following lines will download the adult.data file to the node this notebook is running on from the internet. We can then import it to a dataframe, I have printed the schema which for you to review the headings and get some idea of the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e0ea96-4a17-4381-baf4-1589d0242c1d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note how we were able to directly load a file we downloaded into a dataframe by telling spark to infer the schema of the data and that the text file had headings.\n",
    "\n",
    "Take a look at the below examples, to aggregate data by group, sort data, describe data.\n",
    "\n",
    "For each example try and guess what it will do based on the code before you run the cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff10eaf3-2b40-4eac-8270-9136e8a0313d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b1a84b-823a-4212-a00d-0063c479afd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f736bd-7fb1-4f11-bc56-aed2be4acad2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6afde26b-79e7-4e7f-a94c-d92a50752c14",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cbda45c-72b4-40a7-a714-aa52b2590a1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f187583-171d-4904-bf31-cae2e0abd02c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d44a9bb-fcf4-479b-9509-5dfefb726e64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Practice exercises\n",
    "\n",
    "Combine the functions demonstarted above to find the occupation with the highest number of incomes that are >50K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecfcbd52-fab1-4068-b75d-51759e621612",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da59b5dd-100b-4a9d-acfd-0b1ff1a63b6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a65e02-fe26-4a06-8f10-d155925cea15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac277ac9-9183-49fb-8b98-2300f36209da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abe335e5-2fec-4ee6-b132-de20b2d8a08c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Which 5 profession works the most hours on average?\n",
    "\n",
    "(You don't need to only display the top 5 professions by gender to answer this question, but you can if you like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8aceda4-e969-4a39-9351-d2fc7d79ce0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acfbf2fe-11e9-43b8-bdff-75bb3095d27c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dfd7a8f-12a9-451d-a413-8c1e5d3dbe6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f91ed3-5161-4ae5-a9a2-4fd241de7956",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Execute SQL and HiveQL Queries on a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b05558-65bd-4504-a421-0284443fc8c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you are experienced with SQL or HiveQL you can take advantage of this knowledge when using spark over big data by writting SQL code to query.\n",
    "\n",
    "To do so, first we can use createOrReplaceTempView(), which creates a temporary view. You may have used views before in SQL but a view is like a tempory table. The DataFrame class provides this function. The life of this view is the same as the SparkSession that creates the DataFrame.\n",
    "\n",
    "Using SQLContext, we can run SQL commands, and using HiveContext, we can run HiveQL queries on these temporary tables.\n",
    "\n",
    "In the preceding section of the tutorial, we created a DataFrame named censusDataFrame. We will create a view from this dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07125e54-6ca6-47c7-8815-501176d265df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e71dd24-6f11-4b7f-96d1-b94aaff033fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can then pass SQL queries through to spark as follows, notice how we need to include native-country due to the hyphen. Note The spark.sql() function returns a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ced7aab1-159e-45bd-84d3-f8de993a72f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a18e4f8-7bb7-4a2b-bd20-d7ba1edffc8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In databricks we can use SQL syntax directly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b9c6d92-0d38-43ee-82a1-efa38f9bf813",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f56282-a0df-4237-9d53-40e2a78a9c69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You might notice the way the results are displayed is different, we can use the display command in databricks to load results into the notebook. This will be useful later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0014fb1-b018-40d4-a8d3-16c27ecb539f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e66a10f7-8712-4732-ac36-6953bc4de867",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##  Perform Data Joining on DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9acc12f8-d400-4cda-aa01-569ed3bd3fe2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Often we’re required to combine information from two or more DataFrames or tables. To\n",
    "do this, we perform a join of DataFrames. Basically, table joining is a SQL term, where we\n",
    "join two or more tables to get denormalized tables. Join operations on two tables are very\n",
    "common in data science.\n",
    "\n",
    "In PySparkSQL, we can perform the following types of joins (the keyword for the join type is in brackets):\n",
    "\n",
    "•\t Inner join (deafult)\n",
    "\n",
    "•\t Left outer join (left_outer)\n",
    "\n",
    "•\t Right outer join (right_outer)\n",
    "\n",
    "•\t Full outer join (outer)\n",
    "\n",
    "![image.png](https://cdn.softwaretestinghelp.com/wp-content/qa/uploads/2019/05/Capture-1.jpg)\n",
    "\n",
    "Below we provide a small example demonstaring the syntax for joining:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766c0d11-56c5-42f5-8616-93580ccd5a47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "valuesA = [('Pirate',1,'purple'),('Monkey',2,'brown'),('Ninja',3,'black'),('Spaghetti',4,'white')]\n",
    "DFTableA = spark.createDataFrame(valuesA,['name','id','colour'])\n",
    " \n",
    "valuesB = [('Rutabaga',5,100),('Pirate',1,150),('Ninja',3,35),('Darth Vader',7,55)]\n",
    "DFTableB = spark.createDataFrame(valuesB,['name','id','price'])\n",
    " \n",
    "DFTableA.show()\n",
    "DFTableB.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be0aae51-e1dd-405a-8b83-f78dc8a4a56f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Also note how in the above code we create a dataframe from a list by just providing the data and the names for the headings. This can be a faster way for creating dataframes than the more invovlved method we started the lab with.\n",
    "\n",
    "Two examples of join are shown below, the first assumes we have the same names for our columns, the second does not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd07bec7-828b-4a0e-8d46-3a43d05fdd4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8981ce6a-12ce-4591-8a96-280b8489faf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d511a7ce-3e0c-4601-8552-220cf364a8d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that we had to specify how we would join the data, we would match by ID on each table.\n",
    "\n",
    "Here we show how to perform another join types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "321e312c-2773-48c5-9e69-e00eb0c97004",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248aa8a2-df3e-4f44-9c11-2fe1a3d89d15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The above join was a left join, make sure you are clear which table was considered the left table when making the join (was it tableA or tableB).\n",
    "\n",
    "Peform a full outer join the two tables, joining on the name column and use how='outer'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81ee18b-691c-4ffa-9f05-ccbc464d0d2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fcb8fce-3dda-4ae3-8229-49ffbdc8032f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Summary\n",
    "We have two ways to perfrom SQL like queries on spark, we can use the PySparkSQL functions or we can just write SQL code and run it on a view we have created.\n",
    "\n",
    "The same query is performed in each approach below (note in the first example you can either use 'where' or 'filter' are these are interchangeable in the PySparkSQL):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4f17d9-4bce-4010-a071-526ac52824b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05a26206-7510-4251-8828-0d4bf91e12fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af8300f-662d-4ecf-9594-c2375bb9f8da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Visualisation\n",
    "\n",
    "Databricks has inbuilt functionality for visualisation of our data.\n",
    "\n",
    "https://docs.databricks.com/visualizations/visualization-types.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7429a337-ef66-4bc2-8909-b071f394c126",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Importing the population vs price data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d5726e7-2069-48e8-b5a0-432c8923adc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"/databricks-datasets/samples/population-vs-price/data_geo.csv\", header=\"true\", inferSchema=\"true\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e2f8654-ec8a-4ab6-a9a0-e17c7c49efd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7651a734-c172-4d9e-bb9c-cd96cf19c8eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "138e4416-069b-4561-bd74-6f3621a2c06d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's select State Code and 2015 median sales price using SQL. Click 'Visualization 1' after running this cell. You will see a Map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43979328-74a5-44ea-958a-b33950d0d164",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bba8396-686d-462d-b43c-3647cb4d3e87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Click 'Visualization 1' after running this cell. You will see a Scatter plot.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "829ec255-28f8-4e0c-a224-8d80195a1a7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1fbb86b-fe39-445d-9181-fd5dd77f697b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Data profiles can also be useful for an intial look at our data: Click 'Data Profile 1' after running this cell. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9ac0824-f7ac-4da3-b356-cec730f3c02a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "008f7746-f414-42f9-8689-126ada13f66b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A few more examples:\n",
    "\n",
    "Heatmap: Click 'Visualization 1' after running this cell. You will see a Heatmap. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8a52d49-85d9-4519-b0ca-ff15e49152a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecc1e141-2b8e-4f56-b569-0e7811ab0ba4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Funnel: Click 'Visualization 1' after running this cell. You will see a Funnel \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc5a0bc-cf79-4412-a7a5-394a3eec973b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03b1c35-e06c-4cad-91a0-87fb98dff15c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Sankey: Click 'Visualization 1' after running this cell. You will see a Sankey. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ece89fc5-3eb3-4fcb-bde1-27eef0aa8e3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8936951-552b-4b34-9171-32ffc24f8394",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Wordcloud: Click on 'Visualization 1' after running this cell. You will see a Wordcloud. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1302560317335036,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "lab5_bda",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
